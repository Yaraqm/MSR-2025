{"cells":[{"cell_type":"code","execution_count":null,"id":"57681512","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-17T23:55:10.724754Z","iopub.status.busy":"2024-06-17T23:55:10.724325Z","iopub.status.idle":"2024-06-17T23:55:11.535256Z","shell.execute_reply":"2024-06-17T23:55:11.534221Z"},"papermill":{"duration":0.819224,"end_time":"2024-06-17T23:55:11.537334","exception":false,"start_time":"2024-06-17T23:55:10.718110","status":"completed"},"tags":[],"id":"57681512","outputId":"98ae1486-b76f-4dcc-b34f-6611c8c6a483"},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/trainingset/train.csv\n","/kaggle/input/testwork/train.csv\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"id":"508a2c0d","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:55:11.547280Z","iopub.status.busy":"2024-06-17T23:55:11.546573Z","iopub.status.idle":"2024-06-17T23:59:00.902339Z","shell.execute_reply":"2024-06-17T23:59:00.901106Z"},"papermill":{"duration":229.36353,"end_time":"2024-06-17T23:59:00.905068","exception":false,"start_time":"2024-06-17T23:55:11.541538","status":"completed"},"tags":[],"id":"508a2c0d","outputId":"0a31a859-8327-40f7-c874-770a6bbb8086"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","cudf 24.4.1 requires cubinlinker, which is not installed.\r\n","cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","cudf 24.4.1 requires ptxcompiler, which is not installed.\r\n","cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","keras-cv 0.9.0 requires keras-core, which is not installed.\r\n","keras-nlp 0.12.1 requires keras-core, which is not installed.\r\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n","apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.23.4 which is incompatible.\r\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\r\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\r\n","cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n","cudf 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\r\n","dask-cudf 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\r\n","distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n","google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\r\n","google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\r\n","google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.0 which is incompatible.\r\n","google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.23.4 which is incompatible.\r\n","google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.23.4 which is incompatible.\r\n","google-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.23.4 which is incompatible.\r\n","jupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n","kfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.23.4 which is incompatible.\r\n","kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.23.4 which is incompatible.\r\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n","preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\r\n","pyopenssl 23.3.0 requires cryptography<42,>=41.0.5, but you have cryptography 42.0.5 which is incompatible.\r\n","rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n","rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n","spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n","tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\r\n","tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.23.4 which is incompatible.\r\n","tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.23.4 which is incompatible.\r\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["import os\n","!pip install -U autotrain-advanced > install_logs.txt\n","!autotrain setup --colab > setup_logs.txt"]},{"cell_type":"code","execution_count":null,"id":"984e58d0","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:00.915139Z","iopub.status.busy":"2024-06-17T23:59:00.914804Z","iopub.status.idle":"2024-06-17T23:59:09.930540Z","shell.execute_reply":"2024-06-17T23:59:09.929573Z"},"papermill":{"duration":9.023415,"end_time":"2024-06-17T23:59:09.932899","exception":false,"start_time":"2024-06-17T23:59:00.909484","status":"completed"},"tags":[],"id":"984e58d0","outputId":"4bccc6f6-61fe-47fc-a521-cb9299942874"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:08\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mInstalling latest xformers\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:08\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSuccessfully installed latest xformers\u001b[0m\r\n"]}],"source":["!autotrain setup"]},{"cell_type":"code","execution_count":null,"id":"ff652968","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:09.943816Z","iopub.status.busy":"2024-06-17T23:59:09.943100Z","iopub.status.idle":"2024-06-17T23:59:09.947593Z","shell.execute_reply":"2024-06-17T23:59:09.946767Z"},"papermill":{"duration":0.0121,"end_time":"2024-06-17T23:59:09.949564","exception":false,"start_time":"2024-06-17T23:59:09.937464","status":"completed"},"tags":[],"id":"ff652968"},"outputs":[],"source":["project_name = 'finetunellama'\n","model_name = 'meta-llama/Llama-2-7b'"]},{"cell_type":"code","execution_count":null,"id":"cd16f708","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:09.959080Z","iopub.status.busy":"2024-06-17T23:59:09.958820Z","iopub.status.idle":"2024-06-17T23:59:09.963900Z","shell.execute_reply":"2024-06-17T23:59:09.963146Z"},"papermill":{"duration":0.011991,"end_time":"2024-06-17T23:59:09.965851","exception":false,"start_time":"2024-06-17T23:59:09.953860","status":"completed"},"tags":[],"id":"cd16f708"},"outputs":[],"source":["push_to_hub = True\n","hf_token = \"insert token\"\n","hf_username = \"insert username\""]},{"cell_type":"code","execution_count":null,"id":"2a3805ff","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:09.975820Z","iopub.status.busy":"2024-06-17T23:59:09.975496Z","iopub.status.idle":"2024-06-17T23:59:09.980932Z","shell.execute_reply":"2024-06-17T23:59:09.980081Z"},"papermill":{"duration":0.01265,"end_time":"2024-06-17T23:59:09.982817","exception":false,"start_time":"2024-06-17T23:59:09.970167","status":"completed"},"tags":[],"id":"2a3805ff"},"outputs":[],"source":["learning_rate = 2e-4\n","num_epochs = 1\n","batch_size = 1\n","block_size = 5\n","trainer = \"sft\"\n","warmup_ratio = 0.1\n","weight_decay = 0.01\n","gradient_accumulation = 4\n","mixed_precision = \"fp16\"\n","peft = True\n","quantization = \"int4\"\n","lora_r = 16\n","lora_alpha = 32\n","lora_dropout = 0.05"]},{"cell_type":"code","execution_count":null,"id":"4894abd8","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:09.992689Z","iopub.status.busy":"2024-06-17T23:59:09.992391Z","iopub.status.idle":"2024-06-17T23:59:10.000876Z","shell.execute_reply":"2024-06-17T23:59:10.000013Z"},"papermill":{"duration":0.015575,"end_time":"2024-06-17T23:59:10.002772","exception":false,"start_time":"2024-06-17T23:59:09.987197","status":"completed"},"tags":[],"id":"4894abd8"},"outputs":[],"source":["import os\n","os.environ[\"PROJECT_NAME\"] = project_name\n","os.environ[\"MODEL_NAME\"] = model_name\n","os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n","os.environ[\"HF_TOKEN\"] = hf_token\n","os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n","os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n","os.environ[\"BATCH_SIZE\"] = str(batch_size)\n","os.environ[\"BLOCK_SIZE\"] = str(block_size)\n","os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n","os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n","os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n","os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n","os.environ[\"PEFT\"] = str(peft)\n","os.environ[\"QUANTIZATION\"] = str(quantization)\n","os.environ[\"LORA_R\"] = str(lora_r)\n","os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n","os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n","os.environ[\"HF_USERNAME\"] = hf_username\n","os.environ[\"TRAINER\"] = trainer"]},{"cell_type":"code","execution_count":null,"id":"6b590be3","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:10.012331Z","iopub.status.busy":"2024-06-17T23:59:10.012035Z","iopub.status.idle":"2024-06-17T23:59:10.016556Z","shell.execute_reply":"2024-06-17T23:59:10.015776Z"},"papermill":{"duration":0.011658,"end_time":"2024-06-17T23:59:10.018666","exception":false,"start_time":"2024-06-17T23:59:10.007008","status":"completed"},"tags":[],"id":"6b590be3","outputId":"8da20cdf-1237-43b7-f6b2-b3240c69be31"},"outputs":[{"name":"stdout","output_type":"stream","text":["['trainingset', 'testwork']\n"]}],"source":["import os\n","print(os.listdir(\"../input\"))"]},{"cell_type":"code","execution_count":null,"id":"44c7559b","metadata":{"execution":{"iopub.execute_input":"2024-06-17T23:59:10.028361Z","iopub.status.busy":"2024-06-17T23:59:10.028083Z","iopub.status.idle":"2024-06-18T00:00:01.795785Z","shell.execute_reply":"2024-06-18T00:00:01.794552Z"},"papermill":{"duration":51.77543,"end_time":"2024-06-18T00:00:01.798331","exception":false,"start_time":"2024-06-17T23:59:10.022901","status":"completed"},"tags":[],"id":"44c7559b","outputId":"f9115609-f487-48c7-b427-9b74e1d08e99"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:16\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\r\n","\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-06-17 23:59:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: config, train, func, version, deploy, backend, inference\u001b[0m\r\n","Saving the dataset (1/1 shards): 100%|████| 3/3 [00:00<00:00, 353.44 examples/s]\r\n","Saving the dataset (1/1 shards): 100%|███| 3/3 [00:00<00:00, 1124.18 examples/s]\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:16\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:16\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'finetunellamatestfinal/training_params.json']\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:16\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'project_name': 'finetunellamatestfinal', 'data_path': 'finetunellamatestfinal/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 5, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': True, 'username': 'yaraq', 'token': '*****', 'unsloth': False}\u001b[0m\r\n","/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n","  warn(\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:39\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:39\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:39\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\r\n","    features: ['autotrain_text'],\r\n","    num_rows: 3\r\n","})\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:39\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\r\n","tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 7.48MB/s]\r\n","tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 8.51MB/s]\r\n","tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 18.5MB/s]\r\n","special_tokens_map.json: 100%|█████████████████| 551/551 [00:00<00:00, 2.94MB/s]\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 1\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 5\u001b[0m\r\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\r\n","  warnings.warn(\r\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n","  warnings.warn(\r\n","config.json: 100%|█████████████████████████████| 608/608 [00:00<00:00, 3.97MB/s]\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m578\u001b[0m - \u001b[1mUnsloth available: False\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m591\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\r\n","\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m631\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m633\u001b[0m - \u001b[1mloading model config...\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:40\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m641\u001b[0m - \u001b[1mloading model...\u001b[0m\r\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\r\n","model.safetensors: 100%|████████████████████| 2.20G/2.20G [00:12<00:00, 171MB/s]\r\n","generation_config.json: 100%|███████████████████| 124/124 [00:00<00:00, 810kB/s]\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:57\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m672\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-17 23:59:57\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mcreating trainer\u001b[0m\r\n","\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-06-17 23:59:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\r\n","  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n","    response.raise_for_status()\r\n","  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n","    raise HTTPError(http_error_msg, response=self)\r\n","requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create\r\n","\r\n","The above exception was the direct cause of the following exception:\r\n","\r\n","Traceback (most recent call last):\r\n","  File \"/opt/conda/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\r\n","    return func(*args, **kwargs)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 28, in train\r\n","    train_sft(config)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/autotrain/trainers/clm/train_clm_sft.py\", line 38, in train\r\n","    callbacks = utils.get_callbacks(config)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/autotrain/trainers/clm/utils.py\", line 554, in get_callbacks\r\n","    callbacks = [UploadLogs(config=config), LossLoggingCallback(), TrainStartCallback()]\r\n","  File \"/opt/conda/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 192, in __init__\r\n","    self.api.create_repo(\r\n","  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n","    return fn(*args, **kwargs)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 3256, in create_repo\r\n","    hf_raise_for_status(r)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\r\n","    raise HfHubHTTPError(str(e), response=response) from e\r\n","huggingface_hub.utils._errors.HfHubHTTPError: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6670cdfd-1797f0142769830634b19115;fbfd2fbd-72c3-4db6-9de3-700ea61b7c15)\r\n","\r\n","You already created this model repo\r\n","\u001b[0m\r\n","\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-06-17 23:59:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1m409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6670cdfd-1797f0142769830634b19115;fbfd2fbd-72c3-4db6-9de3-700ea61b7c15)\r\n","\r\n","You already created this model repo\u001b[0m\r\n","\u001b[1mINFO    \u001b[0m | \u001b[32m2024-06-18 00:00:00\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 204\u001b[0m\r\n"]}],"source":["!autotrain llm \\\n","--train \\\n","--model ${MODEL_NAME} \\\n","--project-name ${PROJECT_NAME} \\\n","--data-path /kaggle/input/testwork \\\n","--text-column text \\\n","--lr ${LEARNING_RATE} \\\n","--batch-size ${BATCH_SIZE} \\\n","--epochs ${NUM_EPOCHS} \\\n","--block-size ${BLOCK_SIZE} \\\n","--warmup-ratio ${WARMUP_RATIO} \\\n","--lora-r ${LORA_R} \\\n","--lora-alpha ${LORA_ALPHA} \\\n","--lora-dropout ${LORA_DROPOUT} \\\n","--weight-decay ${WEIGHT_DECAY} \\\n","--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n","--quantization ${QUANTIZATION} \\\n","--mixed-precision ${MIXED_PRECISION} \\\n","--username ${HF_USERNAME} \\\n","--trainer ${TRAINER} \\\n","$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n","$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN}\" )"]},{"cell_type":"code","execution_count":null,"id":"d20d28c4","metadata":{"execution":{"iopub.execute_input":"2024-06-18T00:00:25.057226Z","iopub.status.busy":"2024-06-18T00:00:25.056432Z","iopub.status.idle":"2024-06-18T00:00:30.777364Z","shell.execute_reply":"2024-06-18T00:00:30.776279Z"},"papermill":{"duration":5.740831,"end_time":"2024-06-18T00:00:30.779669","exception":false,"start_time":"2024-06-18T00:00:25.038838","status":"completed"},"tags":[],"id":"d20d28c4"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","model_path = \"insert model path\"\n","\n","# Load the tokenizer and model from the specified path\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    device_map=\"auto\",\n","    torch_dtype='auto'\n",").eval()\n","\n","# Define the user message with a refined prompt\n","prompt = \"\"\"\n","Write a concise git commit message for the following code change using this structure for the commit message:\n","- A short, descriptive title\n","- A more detailed description on a new line explaining:\n","  - What has been changed\n","  - Why the change was made\n","  - Any additional context or relevant information:\n","\n","Use a professional tone, with a focus on clarity and completeness to improve understanding for future reference.\n","\n","Insert code change.\n","\n","Commit message:\n","\"\"\"\n","\n","# Tokenize the prompt\n","input_ids = tokenizer(prompt, return_tensors='pt')\n","\n","# Generate a response from the model\n","input_ids = input_ids['input_ids'].to('cuda')\n","output_ids = model.generate(input_ids, max_length=1000, temperature=0.7, top_p=0.9, num_return_sequences=1)\n","response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","# Print the model's response\n","print(\"Model response:\", response.strip())\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5223709,"sourceId":8708354,"sourceType":"datasetVersion"},{"datasetId":5223784,"sourceId":8708470,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":324.333141,"end_time":"2024-06-18T00:00:32.120957","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-17T23:55:07.787816","version":"2.5.0"},"colab":{"provenance":[{"file_id":"1h4PoPxWunw1ASHsWW_TYn58jOYuPjj8E","timestamp":1731093229790}]}},"nbformat":4,"nbformat_minor":5}